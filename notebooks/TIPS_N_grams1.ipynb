{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4563d6ed",
   "metadata": {},
   "source": [
    "#  Blues Clues TIPS Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "198854f7",
   "metadata": {},
   "source": [
    "### This is a notebook for analysis of the TIPS dataset provided by the Institute for Advanced Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "be1d5770",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\edbak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\edbak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.probability import FreqDist\n",
    "import matplotlib.pyplot as plt\n",
    "# Remove start and end_tokens when looking at distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f49d0c14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TIP #1</th>\n",
       "      <th>TIP #2</th>\n",
       "      <th>TIP #3</th>\n",
       "      <th>Anything else you want to mention? [this can be more TIPS if you are overflowing with advice]</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Keep in mind before the class: where have we b...</td>\n",
       "      <td>Remember to clean the kitchen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>prioritize life outside of the IAA so you don'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>start the data visualization project way earli...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When they say \"trust the process,\" you actuall...</td>\n",
       "      <td>you are expected to maintain an A/B grade poin...</td>\n",
       "      <td>Fall 2 is the most stressful, fast-paced semes...</td>\n",
       "      <td>Don't worry about the TIPS project so much. It...</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>when you are interested in an analytic group ...</td>\n",
       "      <td>If you are coming into the program after havin...</td>\n",
       "      <td>Depending on your cohort, you may or may not m...</td>\n",
       "      <td>Put your mental health and relationships, abov...</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Take good care of yourself. Getting adequate s...</td>\n",
       "      <td>Don't be shy to ask the faculty, classmates, T...</td>\n",
       "      <td>Hang out with your classmates during lunch or ...</td>\n",
       "      <td>It's useful to have prior knowledge of Python ...</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              TIP #1  \\\n",
       "0  Keep in mind before the class: where have we b...   \n",
       "1  prioritize life outside of the IAA so you don'...   \n",
       "2  When they say \"trust the process,\" you actuall...   \n",
       "3   when you are interested in an analytic group ...   \n",
       "4  Take good care of yourself. Getting adequate s...   \n",
       "\n",
       "                                              TIP #2  \\\n",
       "0                      Remember to clean the kitchen   \n",
       "1                                                NaN   \n",
       "2  you are expected to maintain an A/B grade poin...   \n",
       "3  If you are coming into the program after havin...   \n",
       "4  Don't be shy to ask the faculty, classmates, T...   \n",
       "\n",
       "                                              TIP #3  \\\n",
       "0                                                NaN   \n",
       "1  start the data visualization project way earli...   \n",
       "2  Fall 2 is the most stressful, fast-paced semes...   \n",
       "3  Depending on your cohort, you may or may not m...   \n",
       "4  Hang out with your classmates during lunch or ...   \n",
       "\n",
       "  Anything else you want to mention? [this can be more TIPS if you are overflowing with advice]  \\\n",
       "0                                                NaN                                              \n",
       "1                                                NaN                                              \n",
       "2  Don't worry about the TIPS project so much. It...                                              \n",
       "3  Put your mental health and relationships, abov...                                              \n",
       "4  It's useful to have prior knowledge of Python ...                                              \n",
       "\n",
       "   Year  \n",
       "0  2023  \n",
       "1  2023  \n",
       "2  2023  \n",
       "3  2023  \n",
       "4  2023  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('MSA2024_TIPS_Data.csv')\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de64b406",
   "metadata": {},
   "source": [
    "# Text pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2e607294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text pre-processing, remove stop words etc, lower case\n",
    "# Plot Unigram, bigram, trigram frequencies for each year\n",
    "# IMPORTANT: A 'token' is just a word. We call them tokens because not every item in a sentence is an english word... i.e.\n",
    "# numbers, symbols, etc. When I split things into tokens, I am just breaking up the bigger string\n",
    "\n",
    "# data structure looks like this\n",
    "# years = {2016->2023}\n",
    "# each year is list of lists containing the text tokens\n",
    "# Replace the NaNs with empty string\n",
    "df=df.replace(to_replace = np.nan, value = \" \") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eafeccc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intializing our years dictionary with empty lists that will contain sublists of TIPS as tokens\n",
    "years = {2016: [], 2017: [], 2018: [], 2019: [], 2020: [], 2021: [], 2022: [], 2023: []}\n",
    "\n",
    "# for every row\n",
    "for i in range(len(df)):\n",
    "    # and every column excluding the years\n",
    "    for j in range(len(df.columns) - 1):\n",
    "        # get the tips as lower case tokens\n",
    "        tips = nltk.word_tokenize(df.iloc[i,j].lower())\n",
    "        # remove stopwords and punctuation before adding\n",
    "        # NOTE: consider not removing stopwords, as it doesn't read very well, see output below\n",
    "        stop_words = stopwords.words('english')\n",
    "        clean_tips = [word for word in tips if word not in stop_words and word not in string.punctuation and word != \"n't\" and word != \"'re\" and word != \"'s\"]\n",
    "        # add the tips to the dictionary, only if non-empty\n",
    "        if(clean_tips != []):\n",
    "            years[df.iloc[i,4]].append(clean_tips)\n",
    "\n",
    "# print(years)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6629a9ad",
   "metadata": {},
   "source": [
    "# Getting the unigrams\n",
    "#### (uncomment the print at the end to see the map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3055eb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain Unigram counts\n",
    "unigram_map = {2016: {}, 2017: {}, 2018: {}, 2019: {}, 2020: {}, 2021: {}, 2022: {}, 2023: {}}\n",
    "\n",
    "# go through every year\n",
    "for year in years:\n",
    "    \n",
    "    # get unigrams\n",
    "    unigrams = [word for tips in years[year] for word in tips]\n",
    "    \n",
    "    # get distribution of unigrams\n",
    "    fdist = FreqDist(unigrams)\n",
    "\n",
    "    # total number of unigrams\n",
    "    total_unigrams = fdist.N()\n",
    "\n",
    "    # convert counts to probablities\n",
    "    for unigram, frequency in fdist.items():\n",
    "        prob = frequency / total_unigrams\n",
    "        # add probs to bigram map\n",
    "        unigram_map[year][unigram] = prob\n",
    "        \n",
    "# now we have a unigram probability distribution for every word, for every year, saved in this unigram map!\n",
    "# print(unigram_map)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5744214a",
   "metadata": {},
   "source": [
    "### Creating distribution graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5ee00d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year 2016: Top 10 words by frequency are ['get', 'time', 'learn', 'take', 'make', 'program', 'job', 'work', 'help', 'much']\n",
      "Year 2017: Top 10 words by frequency are ['time', 'get', 'work', 'program', 'make', 'team', 'take', 'learn', 'try', 'people']\n",
      "Year 2018: Top 10 words by frequency are ['time', 'get', 'help', 'learn', 'make', 'team', 'people', 'take', 'try', 'great']\n",
      "Year 2019: Top 10 words by frequency are ['get', 'program', 'work', 'take', 'learn', 'interviews', 'time', 'good', 'practicum', 'know']\n",
      "Year 2020: Top 10 words by frequency are ['time', 'get', 'program', 'people', 'make', 'work', 'take', 'know', 'classmates', 'things']\n",
      "Year 2021: Top 10 words by frequency are ['program', 'get', 'time', 'make', 'take', 'work', 'help', '’', 'learn', 'people']\n",
      "Year 2022: Top 10 words by frequency are ['time', 'get', 'program', 'take', 'make', 'work', 'go', 'job', 'help', 'know']\n",
      "Year 2023: Top 10 words by frequency are ['time', 'program', 'make', 'get', 'work', 'people', 'friends', 'fall', 'take', 'interviews']\n"
     ]
    }
   ],
   "source": [
    "for year, word_freqs in unigram_map.items():\n",
    "    sorted_freqs = sorted(word_freqs.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_10_words = [word for word, freq in sorted_freqs[:10]]\n",
    "    print(f\"Year {year}: Top 10 words by frequency are {top_10_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0492eef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert start and end tokens to know where one tip ends and the next begins\n",
    "for year in years:\n",
    "    for tips in years[year]:\n",
    "        tips.insert(0,\"start_token\")\n",
    "        tips.append(\"end_token\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22d2678a",
   "metadata": {},
   "source": [
    "# Getting the bigrams\n",
    "#### (uncomment the print at the end to see the map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0fe6bb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain Bigram counts\n",
    "\n",
    "# creating the bigram map\n",
    "bigram_map = {2016: {}, 2017: {}, 2018: {}, 2019: {}, 2020: {}, 2021: {}, 2022: {}, 2023: {}}\n",
    "\n",
    "# go through every year\n",
    "for year in years:\n",
    "    \n",
    "    # and flatten the arrays\n",
    "    words = [word for tips in years[year] for word in tips]\n",
    "    \n",
    "    # get bigrams\n",
    "    bigrams = list(nltk.bigrams(words))\n",
    "    \n",
    "    # get distribution of bigrams\n",
    "    fdist = FreqDist(bigrams)\n",
    "\n",
    "    # total number of bigrams\n",
    "    total_bigrams = fdist.N()\n",
    "\n",
    "    # convert counts to probablities\n",
    "    for bigram, frequency in fdist.items():\n",
    "        prob = frequency / total_bigrams\n",
    "        # add probs to bigram map\n",
    "        bigram_map[year][bigram] = prob\n",
    "\n",
    "# now we have a bigram probability distribution for every word, for every year, saved in this bigram map\n",
    "    \n",
    "# print(bigram_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8fe25fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year 2016: Top 10 words by frequency are [('end_token', 'start_token'), ('start_token', 'take'), ('start_token', 'learn'), ('program', 'end_token'), ('make', 'sure'), ('take', 'time'), ('fall', 'semester'), ('job', 'search'), ('start_token', 'make'), ('start_token', 'keep'), ('start_token', 'try'), ('start_token', 'trust'), ('r', 'python'), ('get', 'know'), ('start_token', 'apply')]\n",
      "Year 2017: Top 10 words by frequency are [('end_token', 'start_token'), ('start_token', 'take'), ('make', 'sure'), ('start_token', 'try'), ('get', 'know'), ('take', 'time'), ('program', 'end_token'), ('job', 'search'), ('work', 'end_token'), ('try', 'get'), ('start_token', 'make'), ('take', 'advantage'), ('interview', 'season'), ('process', 'end_token'), ('start_token', 'start')]\n",
      "Year 2018: Top 10 words by frequency are [('end_token', 'start_token'), ('trust', 'process'), ('practicum', 'team'), ('start_token', 'trust'), ('start_token', 'get'), ('start_token', 'take'), ('get', 'know'), ('open', 'source'), ('start_token', 'learn'), ('r', 'python'), ('fall', '3'), ('start_token', 'find'), ('help', 'end_token'), ('side', 'project'), ('great', 'way')]\n",
      "Year 2019: Top 10 words by frequency are [('end_token', 'start_token'), ('get', 'know'), ('start_token', 'take'), ('start_token', 'get'), ('practicum', 'team'), ('start_token', 'learn'), ('start_token', 'keep'), ('interview', 'season'), ('start_token', 'fun'), ('take', 'advantage'), ('start_token', 'start'), ('r', 'python'), ('make', 'sure'), ('fall', '3'), ('start_token', 'put')]\n",
      "Year 2020: Top 10 words by frequency are [('end_token', 'start_token'), ('start_token', 'take'), ('take', 'time'), ('get', 'know'), ('start_token', 'make'), ('trust', 'process'), ('information', 'sessions'), ('start_token', 'get'), ('ask', 'questions'), ('make', 'sure'), ('everyone', 'comes'), ('take', 'advantage'), ('start_token', 'compare'), ('fellow', 'classmates'), ('interviews', 'end_token')]\n",
      "Year 2021: Top 10 words by frequency are [('end_token', 'start_token'), ('start_token', 'take'), ('make', 'sure'), ('start_token', 'make'), ('job', 'search'), ('get', 'know'), ('trust', 'process'), ('feel', 'like'), ('take', 'time'), ('start_token', 'program'), ('data', 'science'), ('start_token', 'get'), ('start_token', 'try'), ('side', 'project'), ('time', 'end_token')]\n",
      "Year 2022: Top 10 words by frequency are [('end_token', 'start_token'), ('start_token', 'take'), ('make', 'sure'), ('job', 'search'), ('social', 'events'), ('interview', 'season'), ('get', 'know'), ('know', 'classmates'), ('start_token', 'make'), ('take', 'time'), ('make', 'time'), ('winter', 'break'), ('start_token', 'get'), ('spring', '1'), ('take', 'advantage')]\n",
      "Year 2023: Top 10 words by frequency are [('end_token', 'start_token'), ('start_token', 'make'), ('trust', 'process'), ('start_token', 'take'), ('make', 'sure'), ('get', 'know'), ('side', 'project'), ('practicum', 'project'), ('take', 'time'), ('interviews', 'end_token'), ('fall', '2'), ('``', 'trust'), ('process', \"''\"), ('mental', 'health'), ('process', 'end_token')]\n"
     ]
    }
   ],
   "source": [
    "for year, word_freqs in bigram_map.items():\n",
    "    sorted_freqs = sorted(word_freqs.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_10_words = [word for word, freq in sorted_freqs[:15]]\n",
    "    print(f\"Year {year}: Top 10 words by frequency are {top_10_words}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb3a5b91",
   "metadata": {},
   "source": [
    "# Getting the trigrams\n",
    "#### (uncomment the print at the end to see the map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415aa5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain Trigram counts\n",
    "\n",
    "# creating the trigram map\n",
    "trigram_map = {2016: {}, 2017: {}, 2018: {}, 2019: {}, 2020: {}, 2021: {}, 2022: {}, 2023: {}}\n",
    "\n",
    "# go through every year\n",
    "for year in years:\n",
    "    \n",
    "    # and flatten the arrays\n",
    "    words = [word for tips in years[year] for word in tips]\n",
    "    \n",
    "    # get trigrams\n",
    "    trigrams = list(nltk.trigrams(words))\n",
    "    \n",
    "    # get distribution of trigrams\n",
    "    fdist = FreqDist(trigrams)\n",
    "\n",
    "    # total number of trigrams\n",
    "    total_trigrams = fdist.N()\n",
    "\n",
    "    # convert counts to probablities\n",
    "    for trigram, frequency in fdist.items():\n",
    "        prob = frequency / total_trigrams\n",
    "        # add probs to trigram map\n",
    "        trigram_map[year][trigram] = prob\n",
    "\n",
    "# now we have a trigram probability distribution for every word, for every year, saved in this trigram map!\n",
    "# print(trigram_map)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
