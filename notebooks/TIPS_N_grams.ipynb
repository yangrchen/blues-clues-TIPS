{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4563d6ed",
   "metadata": {},
   "source": [
    "#  Blues Clues TIPS Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198854f7",
   "metadata": {},
   "source": [
    "### This is a notebook for analysis of the TIPS dataset provided by the Institute for Advanced Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1d5770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49d0c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('MSA2024_TIPS_Data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de64b406",
   "metadata": {},
   "source": [
    "# Text pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e607294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text pre-processing, remove stop words etc, lower case\n",
    "# Plot Unigram, bigram, trigram frequencies for each year\n",
    "# IMPORTANT: A 'token' is just a word. We call them tokens because not every item in a sentence is an english word... i.e.\n",
    "# numbers, symbols, etc. When I split things into tokens, I am just breaking up the bigger string\n",
    "\n",
    "# data structure looks like this\n",
    "# years = {2016->2023}\n",
    "# each year is list of lists containing the text tokens\n",
    "# Replace the NaNs with empty string\n",
    "df=df.replace(to_replace = np.nan, value = \" \") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafeccc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intializing our years dictionary with empty lists that will contain sublists of TIPS as tokens\n",
    "years = {2016: [], 2017: [], 2018: [], 2019: [], 2020: [], 2021: [], 2022: [], 2023: []}\n",
    "\n",
    "# for every row\n",
    "for i in range(len(df)):\n",
    "    # and every column excluding the years\n",
    "    for j in range(len(df.columns) - 1):\n",
    "        # get the tips as lower case tokens\n",
    "        tips = nltk.word_tokenize(df.iloc[i,j].lower())\n",
    "        # remove stopwords and punctuation before adding\n",
    "        # NOTE: consider not removing stopwords, as it doesn't read very well, see output below\n",
    "        stop_words = stopwords.words('english')\n",
    "        clean_tips = [word for word in tips if word not in stop_words and word not in string.punctuation]\n",
    "        # add the tips to the dictionary\n",
    "        years[df.iloc[i,4]].append(clean_tips)\n",
    "\n",
    "# insert start and end tokens to know where one tip ends and the next begins\n",
    "for year in years:\n",
    "    for tips in years[year]:\n",
    "        tips.insert(0,\"start_token\")\n",
    "        tips.append(\"end_token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6629a9ad",
   "metadata": {},
   "source": [
    "# Getting the unigrams\n",
    "#### (uncomment the print at the end to see the map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3055eb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain Unigram counts\n",
    "unigram_map = {2016: {}, 2017: {}, 2018: {}, 2019: {}, 2020: {}, 2021: {}, 2022: {}, 2023: {}}\n",
    "\n",
    "# go through every year\n",
    "for year in years:\n",
    "    \n",
    "    # get unigrams\n",
    "    unigrams = [word for tips in years[year] for word in tips]\n",
    "    \n",
    "    # get distribution of unigrams\n",
    "    fdist = FreqDist(unigrams)\n",
    "\n",
    "    # total number of unigrams\n",
    "    total_unigrams = fdist.N()\n",
    "\n",
    "    # convert counts to probablities\n",
    "    for unigram, frequency in fdist.items():\n",
    "        prob = frequency / total_unigrams\n",
    "        # add probs to bigram map\n",
    "        unigram_map[year][unigram] = prob\n",
    "        \n",
    "# now we have a unigram probability distribution for every word, for every year, saved in this unigram map!\n",
    "# print(unigram_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d2678a",
   "metadata": {},
   "source": [
    "# Getting the bigrams\n",
    "#### (uncomment the print at the end to see the map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe6bb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain Bigram counts\n",
    "\n",
    "# creating the bigram map\n",
    "bigram_map = {2016: {}, 2017: {}, 2018: {}, 2019: {}, 2020: {}, 2021: {}, 2022: {}, 2023: {}}\n",
    "\n",
    "# go through every year\n",
    "for year in years:\n",
    "    \n",
    "    # and flatten the arrays\n",
    "    words = [word for tips in years[year] for word in tips]\n",
    "    \n",
    "    # get bigrams\n",
    "    bigrams = list(nltk.bigrams(words))\n",
    "    \n",
    "    # get distribution of bigrams\n",
    "    fdist = FreqDist(bigrams)\n",
    "\n",
    "    # total number of bigrams\n",
    "    total_bigrams = fdist.N()\n",
    "\n",
    "    # convert counts to probablities\n",
    "    for bigram, frequency in fdist.items():\n",
    "        prob = frequency / total_bigrams\n",
    "        # add probs to bigram map\n",
    "        bigram_map[year][bigram] = prob\n",
    "\n",
    "# now we have a bigram probability distribution for every word, for every year, saved in this bigram map!\n",
    "# print(bigram_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3a5b91",
   "metadata": {},
   "source": [
    "# Getting the trigrams\n",
    "#### (uncomment the print at the end to see the map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415aa5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain Trigram counts\n",
    "\n",
    "# creating the trigram map\n",
    "trigram_map = {2016: {}, 2017: {}, 2018: {}, 2019: {}, 2020: {}, 2021: {}, 2022: {}, 2023: {}}\n",
    "\n",
    "# go through every year\n",
    "for year in years:\n",
    "    \n",
    "    # and flatten the arrays\n",
    "    words = [word for tips in years[year] for word in tips]\n",
    "    \n",
    "    # get trigrams\n",
    "    trigrams = list(nltk.trigrams(words))\n",
    "    \n",
    "    # get distribution of trigrams\n",
    "    fdist = FreqDist(trigrams)\n",
    "\n",
    "    # total number of trigrams\n",
    "    total_trigrams = fdist.N()\n",
    "\n",
    "    # convert counts to probablities\n",
    "    for trigram, frequency in fdist.items():\n",
    "        prob = frequency / total_trigrams\n",
    "        # add probs to trigram map\n",
    "        trigram_map[year][trigram] = prob\n",
    "\n",
    "# now we have a trigram probability distribution for every word, for every year, saved in this trigram map!\n",
    "# print(trigram_map)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
